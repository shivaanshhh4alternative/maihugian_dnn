{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Experiment - 5**"],"metadata":{"id":"aI8ZGzP2H6R7"}},{"cell_type":"markdown","metadata":{"id":"eBqNMeZADPGk"},"source":["# **Text data handling with RNN for sentiment analysis**"]},{"cell_type":"markdown","source":["‎\n","‎\n","‎\n","‎\n","‎\n","‎\n","‎ ‎\n","‎\n","\n","‎\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"1TsH-bG5L3ZK"}},{"cell_type":"markdown","source":["#**Importing Necessary libraries**\n"],"metadata":{"id":"iJ0ZKrDxffoG"}},{"cell_type":"code","source":["import pandas as pd    # to load dataset\n","import numpy as np     # for mathematic equation\n","from nltk.corpus import stopwords   # to get collection of stopwords\n","from sklearn.model_selection import train_test_split       # for splitting dataset\n","from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n","from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n","from tensorflow.keras.models import Sequential     # the model\n","from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n","from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n","from tensorflow.keras.models import load_model   # load saved model\n","import re\n","from keras.layers import SimpleRNN"],"metadata":{"id":"7iXDJSIGjjn-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Preparing the data named IMDB**"],"metadata":{"id":"yzRVuD3qkGvD"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/AMITY/Deep Learning (codes)/Data/IMDB Dataset.csv')\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RI03JfP7kEKA","executionInfo":{"status":"ok","timestamp":1684231839549,"user_tz":-330,"elapsed":1681,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"7d9cfb0e-5ac8-4c11-ce7f-5fd56b2a28cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]\n"]}]},{"cell_type":"markdown","source":["Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n","Declaring the english stop words"],"metadata":{"id":"B4KcdMAKtQan"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"stopwords\")\n","english_stops = set(stopwords.words('english'))"],"metadata":{"id":"xjJrIijipG5D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684231839549,"user_tz":-330,"elapsed":5,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"ebbb2feb-153b-4600-daf4-bc69a2120043"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["# **Load and Clean Dataset**\n","**In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.**\n","\n","# **Encode Sentiments**\n","**In the same function, We also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments.**"],"metadata":{"id":"PlWrgDKttZC1"}},{"cell_type":"code","source":["def load_dataset():\n","    df = pd.read_csv('/content/drive/MyDrive/AMITY/Deep Learning (codes)/Data/IMDB Dataset.csv')\n","    x_data = df['review']       # Reviews/Input\n","    y_data = df['sentiment']    # Sentiment/Output\n","\n","    # PRE-PROCESS REVIEW\n","    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n","    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n","    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n","    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n","\n","    # ENCODE SENTIMENT -> 0 & 1\n","    y_data = y_data.replace('positive', 1)\n","    y_data = y_data.replace('negative', 0)\n","\n","    return x_data, y_data\n","\n","x_data, y_data = load_dataset()\n","\n","print('Reviews')\n","print(x_data, '\\n')\n","print('Sentiment')\n","print(y_data)"],"metadata":{"id":"NM_5RjUNqa-s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684231847882,"user_tz":-330,"elapsed":8335,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"30d7113d-cc6c-4202-b18c-97da68053e0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reviews\n","0        [one, reviewers, mentioned, watching, oz, epis...\n","1        [a, wonderful, little, production, the, filmin...\n","2        [i, thought, wonderful, way, spend, time, hot,...\n","3        [basically, family, little, boy, jake, thinks,...\n","4        [petter, mattei, love, time, money, visually, ...\n","                               ...                        \n","49995    [i, thought, movie, right, good, job, it, crea...\n","49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n","49997    [i, catholic, taught, parochial, elementary, s...\n","49998    [i, going, disagree, previous, comment, side, ...\n","49999    [no, one, expects, star, trek, movies, high, a...\n","Name: review, Length: 50000, dtype: object \n","\n","Sentiment\n","0        1\n","1        1\n","2        1\n","3        0\n","4        1\n","        ..\n","49995    1\n","49996    0\n","49997    0\n","49998    0\n","49999    0\n","Name: sentiment, Length: 50000, dtype: int64\n"]}]},{"cell_type":"markdown","source":["#**Split Dataset**\n","**In this work, We decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions.**"],"metadata":{"id":"KcQbFcH0thD1"}},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n","\n","print('Train Set')\n","print(x_train, '\\n')\n","print(x_test, '\\n')\n","print('Test Set')\n","print(y_train, '\\n')\n","print(y_test)"],"metadata":{"id":"8cy2sU3jthtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684231847883,"user_tz":-330,"elapsed":16,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"14a08333-0159-4fa5-a751-e51a990df70d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Set\n","22596    [boring, badly, written, italian, exploitation...\n","5353     [the, other, supposed, horror, movie, made, it...\n","42152    [a, tough, life, gets, tougher, three, childre...\n","15434    [why, earth, colin, firth, pointless, film, ha...\n","7280     [this, far, worst, movie, i, ever, seen, cinem...\n","                               ...                        \n","39945    [this, show, lasted, moments, plots, usually, ...\n","13858    [i, rented, thinking, would, pretty, good, cov...\n","25266    [having, pleasantly, surprised, sandra, bulloc...\n","10659    [the, difficulty, i, musical, version, les, mi...\n","39372    [this, movie, proof, film, noire, enduring, st...\n","Name: review, Length: 40000, dtype: object \n","\n","2006     [this, movie, time, favorite, you, really, see...\n","33575    [this, british, film, version, stage, play, i,...\n","6808     [alexander, nevsky, brilliant, piece, cinemati...\n","32330    [found, old, vhs, version, film, parents, hous...\n","3777     [i, went, see, movie, daughter, i, insisted, g...\n","                               ...                        \n","40255    [what, heck, people, expect, horror, films, da...\n","5864     [especially, time, much, science, fiction, fil...\n","44604    [nicole, eggert, listed, star, despite, michea...\n","42481    [a, thief, night, got, best, end, times, thril...\n","31671    [i, enjoy, national, anthem, i, enjoy, nationa...\n","Name: review, Length: 10000, dtype: object \n","\n","Test Set\n","22596    0\n","5353     0\n","42152    1\n","15434    0\n","7280     0\n","        ..\n","39945    0\n","13858    0\n","25266    0\n","10659    0\n","39372    1\n","Name: sentiment, Length: 40000, dtype: int64 \n","\n","2006     1\n","33575    1\n","6808     1\n","32330    0\n","3777     0\n","        ..\n","40255    1\n","5864     1\n","44604    0\n","42481    1\n","31671    1\n","Name: sentiment, Length: 10000, dtype: int64\n"]}]},{"cell_type":"markdown","source":["**Function for getting the average review length, by calculating the mean of all the reviews length (using numpy.mean)**"],"metadata":{"id":"B712-ID4tnDY"}},{"cell_type":"code","source":["def get_max_length():\n","    review_length = []\n","    for review in x_train:\n","        review_length.append(len(review))\n","\n","    return int(np.ceil(np.mean(review_length)))"],"metadata":{"id":"5n86Ria5toOI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Tokenize and Pad/Truncate Reviews**\n","**A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.**\n","\n","**x_train and x_test is converted into integers using texts_to_sequences method.**\n","\n","**Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.**"],"metadata":{"id":"zwYgTbAEts_G"}},{"cell_type":"code","source":["# ENCODE REVIEW\n","token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n","token.fit_on_texts(x_train)\n","x_train = token.texts_to_sequences(x_train)\n","x_test = token.texts_to_sequences(x_test)\n","\n","max_length = get_max_length()\n","\n","x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n","x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n","\n","total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n","print('Total Words:', total_words)\n","\n","print('Encoded X Train\\n', x_train, '\\n')\n","print('Encoded X Test\\n', x_test, '\\n')\n","print('Maximum review length: ', max_length)"],"metadata":{"id":"_cRXNvFHttoe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684231852973,"user_tz":-330,"elapsed":5100,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"70e45442-4ac0-4fc8-8971-808224a57fe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Words: 92636\n","Encoded X Train\n"," [[  257   863   310 ...     0     0     0]\n"," [    2  1340   350 ...    28   282   409]\n"," [   39  1138    40 ...     0     0     0]\n"," ...\n"," [ 1587  3903   660 ...    62 14457  1006]\n"," [    2  6090     1 ...  4973  5675   406]\n"," [    8     3  2912 ...     0     0     0]] \n","\n","Encoded X Test\n"," [[    8     3    10 ...     0     0     0]\n"," [    8   603     4 ...   278 10278  2289]\n"," [ 3551 11276   417 ...     0     0     0]\n"," ...\n"," [ 3908 20405  3718 ...     0     0     0]\n"," [   39  2984   218 ...  3947     3   765]\n"," [    1   260  1833 ...     0     0     0]] \n","\n","Maximum review length:  130\n"]}]},{"cell_type":"markdown","source":["#**Build Architecture/Model**\n","**Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.**\n","\n","**RNN Layer: to make a decision to keep or throw away data by considering the current input, previous output.**\n","\n","**Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.**\n","\n","**The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number.**"],"metadata":{"id":"eXf1daJctzRK"}},{"cell_type":"code","source":["rnn = Sequential()\n","\n","rnn.add(Embedding(total_words,32,input_length =max_length))\n","rnn.add(SimpleRNN(64,input_shape = (total_words, max_length), return_sequences=False,activation=\"relu\"))\n","rnn.add(Dense(1, activation = 'sigmoid')) #flatten\n","\n","print(rnn.summary())\n","rnn.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"],"metadata":{"id":"FoM7MNAKtz50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684231857441,"user_tz":-330,"elapsed":4478,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"1971c951-a587-4f37-9632-481b5fc6a896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 130, 32)           2964352   \n","                                                                 \n"," simple_rnn (SimpleRNN)      (None, 64)                6208      \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,970,625\n","Trainable params: 2,970,625\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["#**Trainin the Model**"],"metadata":{"id":"aqi0Y67xFeyZ"}},{"cell_type":"code","source":["history = rnn.fit(x_train,y_train,epochs = 20,batch_size=128,verbose = 1)"],"metadata":{"id":"udaR9DEFu6Fr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d7f4c98-14af-4fd1-d108-3457ab8e859b","executionInfo":{"status":"ok","timestamp":1684232900661,"user_tz":-330,"elapsed":1043227,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","313/313 [==============================] - 96s 286ms/step - loss: 0.6915 - accuracy: 0.5184\n","Epoch 2/20\n","313/313 [==============================] - 68s 218ms/step - loss: 0.6616 - accuracy: 0.5879\n","Epoch 3/20\n","313/313 [==============================] - 63s 202ms/step - loss: 0.6626 - accuracy: 0.5762\n","Epoch 4/20\n","313/313 [==============================] - 56s 180ms/step - loss: 0.5900 - accuracy: 0.6328\n","Epoch 5/20\n","313/313 [==============================] - 51s 162ms/step - loss: 0.4166 - accuracy: 0.8135\n","Epoch 6/20\n","313/313 [==============================] - 51s 161ms/step - loss: 0.3511 - accuracy: 0.8806\n","Epoch 7/20\n","313/313 [==============================] - 47s 149ms/step - loss: 0.2362 - accuracy: 0.9194\n","Epoch 8/20\n","313/313 [==============================] - 46s 148ms/step - loss: 0.1676 - accuracy: 0.9421\n","Epoch 9/20\n","313/313 [==============================] - 45s 144ms/step - loss: 0.3168 - accuracy: 0.8689\n","Epoch 10/20\n","313/313 [==============================] - 46s 148ms/step - loss: 0.5571 - accuracy: 0.6458\n","Epoch 11/20\n","313/313 [==============================] - 44s 141ms/step - loss: 0.4791 - accuracy: 0.7574\n","Epoch 12/20\n","313/313 [==============================] - 45s 144ms/step - loss: 0.2817 - accuracy: 0.9088\n","Epoch 13/20\n","313/313 [==============================] - 43s 139ms/step - loss: 0.4030 - accuracy: 0.8431\n","Epoch 14/20\n","313/313 [==============================] - 45s 142ms/step - loss: 0.2630 - accuracy: 0.9154\n","Epoch 15/20\n","313/313 [==============================] - 43s 138ms/step - loss: 0.2351 - accuracy: 0.9260\n","Epoch 16/20\n","313/313 [==============================] - 43s 138ms/step - loss: 0.2000 - accuracy: 0.9385\n","Epoch 17/20\n","313/313 [==============================] - 45s 143ms/step - loss: 0.1599 - accuracy: 0.9498\n","Epoch 18/20\n","313/313 [==============================] - 44s 140ms/step - loss: 0.1374 - accuracy: 0.9577\n","Epoch 19/20\n","313/313 [==============================] - 44s 141ms/step - loss: 0.1331 - accuracy: 0.9611\n","Epoch 20/20\n","313/313 [==============================] - 43s 136ms/step - loss: 0.2814 - accuracy: 0.8869\n"]}]},{"cell_type":"markdown","source":["#**Saving The Model**"],"metadata":{"id":"9h8KUOGpFhz2"}},{"cell_type":"code","source":["model = rnn.save('rnn.h5')\n","loaded_model = load_model('rnn.h5')"],"metadata":{"id":"PGQ5CGWd7pLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Evaluation**"],"metadata":{"id":"8fKgbBP4Flqd"}},{"cell_type":"code","source":["y_pred = rnn.predict(x_test, batch_size = 128)\n","print(y_pred)\n","print(y_test)\n","for i in range(len(y_pred)):\n","  if y_pred[i]>0.5:\n","    y_pred[i] = 1\n","  else:\n","    y_pred[i] = 0\n","\n","true = 0\n","for i, y in enumerate(y_test):\n","    if y == y_pred[i]:\n","        true += 1\n","\n","print('Correct Prediction: {}'.format(true))\n","print('Wrong Prediction: {}'.format(len(y_pred) - true))\n","print('Accuracy: {}'.format(true/len(y_pred)*100))"],"metadata":{"id":"t6nkcsHsvMyI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232902285,"user_tz":-330,"elapsed":1627,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"5e99d090-0638-4a7a-b8b9-6ac25aa93132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["79/79 [==============================] - 1s 12ms/step\n","[[0.78446704]\n"," [0.02569966]\n"," [0.78301245]\n"," ...\n"," [0.2700789 ]\n"," [0.72713566]\n"," [0.78446704]]\n","2006     1\n","33575    1\n","6808     1\n","32330    0\n","3777     0\n","        ..\n","40255    1\n","5864     1\n","44604    0\n","42481    1\n","31671    1\n","Name: sentiment, Length: 10000, dtype: int64\n","Correct Prediction: 6918\n","Wrong Prediction: 3082\n","Accuracy: 69.17999999999999\n"]}]},{"cell_type":"markdown","source":["Message: **Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!**"],"metadata":{"id":"xb1KdQ_pNJIW"}},{"cell_type":"markdown","source":["#**Example review**"],"metadata":{"id":"0yN_6mY24Jlw"}},{"cell_type":"code","source":["review = str(input('Movie Review: '))"],"metadata":{"id":"1_lSReZWvapL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232906077,"user_tz":-330,"elapsed":3796,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"421e0566-a8fb-4bf7-fc8e-9332436c8bb9"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Movie Review: Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!\n"]}]},{"cell_type":"markdown","source":["#**Pre-processing of entered review**"],"metadata":{"id":"7VTkE6KF4P31"}},{"cell_type":"code","source":["# Pre-process input\n","regex = re.compile(r'[^a-zA-Z\\s]')\n","review = regex.sub('', review)\n","print('Cleaned: ', review)\n","\n","words = review.split(' ')\n","filtered = [w for w in words if w not in english_stops]\n","filtered = ' '.join(filtered)\n","filtered = [filtered.lower()]\n","\n","print('Filtered: ', filtered)"],"metadata":{"id":"S-isbUeevaq5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232906077,"user_tz":-330,"elapsed":12,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"3bf01824-0d16-41cd-b37c-03b17417e6bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned:  Nothing was typical about this Everything was beautifully done in this movie the story the flow the scenario everything I highly recommend it for mystery lovers for anyone who wants to watch a good movie\n","Filtered:  ['nothing typical everything beautifully done movie story flow scenario everything i highly recommend mystery lovers anyone wants watch good movie']\n"]}]},{"cell_type":"code","source":["tokenize_words = token.texts_to_sequences(filtered)\n","tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n","print(tokenize_words)"],"metadata":{"id":"Te3OyQohvaua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232906077,"user_tz":-330,"elapsed":10,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"6375cb0c-9cf0-4ff0-9d39-b497f0708bce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  76  705  174 1210  126    3   13 2692 2596  174    1  442  280  701\n","  1771  155  400   33    9    3    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0]]\n"]}]},{"cell_type":"markdown","source":["#**Prediction**"],"metadata":{"id":"9X_Haz8g4U7Q"}},{"cell_type":"code","source":["result = rnn.predict(tokenize_words)\n","print(result)"],"metadata":{"id":"vVWAZXuZviIp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232906077,"user_tz":-330,"elapsed":8,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"a721b9ae-3347-468a-ccd1-d9c1c6bf6619"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 40ms/step\n","[[0.78446704]]\n"]}]},{"cell_type":"code","source":["if result >= 0.7:\n","    print('positive')\n","else:\n","    print('negative')"],"metadata":{"id":"gVgQvGXBviME","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684232906078,"user_tz":-330,"elapsed":7,"user":{"displayName":"KARNATI MOHAN","userId":"16604712269571437600"}},"outputId":"c1e1e527-cd1e-4a41-9e6b-e0fabb1e9b10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["positive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gcagsy1Q8LUz"},"execution_count":null,"outputs":[]}]}